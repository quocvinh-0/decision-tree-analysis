"""
MAIN.PY Cáº¢I THIá»†N - Sá»¬ Dá»¤NG PHÆ¯Æ NG PHÃP Tá»I Æ¯U

CÃ¡c cáº£i thiá»‡n:
1. Sá»­ dá»¥ng code improved (khÃ´ng scaling, cÃ³ validation set, GridSearchCV, pruning)
2. BÃ¡o cÃ¡o káº¿t quáº£ rÃµ rÃ ng vÃ  Ä‘áº§y Ä‘á»§
3. PhÃ¹ há»£p cho bÃ i bÃ¡o cÃ¡o khoa há»c
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
import matplotlib
matplotlib.use('Agg')  # Sá»­ dá»¥ng backend non-interactive Ä‘á»ƒ trÃ¡nh lá»—i tkinter
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
from scipy import stats
import warnings
import sys
import io
warnings.filterwarnings('ignore')

# Fix encoding for Windows console
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Import cÃ¡c module cáº£i thiá»‡n
from improved.data_loader_improved import load_and_prepare_data, get_scaled_features
from improved.model_trainer_improved import train_decision_trees_improved, calculate_metrics

# Import cÃ¡c module gá»‘c (cáº§n sá»­a Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch)
from model_comparison import compare_with_other_models
from visualization import create_all_visualizations

# Cáº­p nháº­t model_comparison Ä‘á»ƒ sá»­ dá»¥ng calculate_metrics tá»« improved
import model_comparison
model_comparison.calculate_metrics = calculate_metrics

def main():
    """HÃ m chÃ­nh Ä‘á»ƒ cháº¡y toÃ n bá»™ quy trÃ¬nh phÃ¢n tÃ­ch vá»›i phÆ°Æ¡ng phÃ¡p cáº£i thiá»‡n"""
    
    print("="*70)
    print("PHÃ‚N TÃCH Dá»® LIá»†U Vá»šI CÃ‚Y QUYáº¾T Äá»ŠNH - PHÆ¯Æ NG PHÃP Cáº¢I THIá»†N")
    print("="*70)
    print("ğŸ“‹ Dataset: Folds5x2_pp.xlsx (Combined Cycle Power Plant Data)")
    print("ğŸ¯ Má»¥c tiÃªu: Dá»± Ä‘oÃ¡n sáº£n lÆ°á»£ng Ä‘iá»‡n (PE) tá»« cÃ¡c Ä‘áº·c trÆ°ng mÃ´i trÆ°á»ng")
    print("="*70)
    
    # ============================
    # Táº O THÆ¯ Má»¤C LÆ¯U TRá»®
    # ============================
    os.makedirs('img', exist_ok=True)
    os.makedirs('result', exist_ok=True)
    os.makedirs('report', exist_ok=True)
    print("\nâœ… ÄÃ£ táº¡o thÆ° má»¥c: 'img/', 'result/', 'report/'")
    
    # ============================
    # BÆ¯á»šC 1: Äá»ŒC VÃ€ TIá»€N Xá»¬ LÃ Dá»® LIá»†U
    # ============================
    print("\n" + "="*70)
    print("BÆ¯á»šC 1: Äá»ŒC VÃ€ TIá»€N Xá»¬ LÃ Dá»® LIá»†U")
    print("="*70)
    
    dataset_path = 'Folds5x2_pp.xlsx'
    X, y = load_and_prepare_data(dataset_path, use_enhanced_features=False)
    
    # Táº¡o biá»ƒu Ä‘á»“ phÃ¢n phá»‘i PE cho slide
    from visualization import create_pe_distribution_slide
    create_pe_distribution_slide(y)
    
    # Táº¡o cÃ¢y quyáº¿t Ä‘á»‹nh tá»« 10 máº«u Ä‘áº§u (cho slide)
    from visualization import plot_decision_tree_slide
    plot_decision_tree_slide(X, y)
    
    # Láº¥y scaler cho Naive Bayes (náº¿u cáº§n)
    X_scaled, scaler = get_scaled_features(X)
    
    # ============================
    # BÆ¯á»šC 2: HUáº¤N LUYá»†N MÃ” HÃŒNH DECISION TREE (Cáº¢I THIá»†N)
    # ============================
    print("\n" + "="*70)
    print("BÆ¯á»šC 2: HUáº¤N LUYá»†N MÃ” HÃŒNH DECISION TREE")
    print("="*70)
    print("ğŸ“Œ PhÆ°Æ¡ng phÃ¡p:")
    print("   â€¢ Sá»­ dá»¥ng GridSearchCV Ä‘á»ƒ tÃ¬m hyperparameter tá»‘i Æ°u")
    print("   â€¢ Train/Test split (80/20)")
    print("   â€¢ Cost Complexity Pruning Ä‘á»ƒ giáº£m overfitting")
    print("   â€¢ KhÃ´ng sá»­ dá»¥ng scaling (Decision Tree khÃ´ng cáº§n)")
    print("   â€¢ Chá»n mÃ´ hÃ¬nh dá»±a trÃªn test set")
    
    train_df, test_df, feature_importance_df, best_models, best_model_info = \
        train_decision_trees_improved(X, y, n_runs=10, use_grid_search=True)
    
    # ============================
    # BÆ¯á»šC 3: SO SÃNH Vá»šI MÃ” HÃŒNH KHÃC
    # ============================
    print("\n" + "="*70)
    print("BÆ¯á»šC 3: SO SÃNH Vá»šI CÃC MÃ” HÃŒNH KHÃC")
    print("="*70)
    
    # Chuáº©n bá»‹ dá»¯ liá»‡u cho so sÃ¡nh
    # Sá»­ dá»¥ng train Ä‘á»ƒ train cÃ¡c mÃ´ hÃ¬nh khÃ¡c
    X_train_best = best_model_info['X_train']
    X_test_best = best_model_info['X_test']
    y_train_best = best_model_info['y_train']
    y_test_best = best_model_info['y_test']
    
    # Sá»­ dá»¥ng train data Ä‘á»ƒ train cÃ¡c mÃ´ hÃ¬nh khÃ¡c
    if isinstance(X_train_best, pd.DataFrame):
        X_train_combined = X_train_best.values
    else:
        X_train_combined = X_train_best
    
    if isinstance(y_train_best, pd.Series):
        y_train_combined = y_train_best.values
    else:
        y_train_combined = y_train_best
    
    # Chuáº©n hÃ³a dá»¯ liá»‡u cho Naive Bayes (náº¿u cáº§n)
    # Bá» KNN nÃªn khÃ´ng cáº§n scaling ná»¯a, nhÆ°ng Naive Bayes váº«n cáº§n
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_combined)
    X_test_scaled = scaler.transform(X_test_best)
    
    comparison_results = compare_with_other_models(
        X_train_best=X_train_combined,
        X_test_best=X_test_best,
        y_train_best=y_train_combined,
        y_test_best=y_test_best,
        best_model=best_model_info['model'],
        X_train_scaled=X_train_scaled,  # Dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a cho Naive Bayes
        X_test_scaled=X_test_scaled     # Dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a cho Naive Bayes
    )
    
    # ============================
    # BÆ¯á»šC 4: TRá»°C QUAN HÃ“A Káº¾T QUáº¢
    # ============================
    print("\n" + "="*70)
    print("BÆ¯á»šC 4: TRá»°C QUAN HÃ“A Káº¾T QUáº¢")
    print("="*70)
    
    # Cáº­p nháº­t best_model_info Ä‘á»ƒ cÃ³ X_scaled cho visualization
    best_model_info_vis = best_model_info.copy()
    best_model_info_vis['X_scaled'] = X_scaled
    
    create_all_visualizations(
        train_df, test_df, feature_importance_df, best_model_info_vis, 
        comparison_results, X_scaled, y
    )
    
    # Táº¡o thÃªm biá»ƒu Ä‘á»“ train/test comparison
    create_validation_visualizations(train_df, test_df, best_model_info)
    
    # Táº¡o biá»ƒu Ä‘á»“ Decision Tree lÃ  tá»‘t nháº¥t
    create_decision_tree_best_charts(comparison_results)
    
    # Táº¡o cÃ¢y quyáº¿t Ä‘á»‹nh rÃºt gá»n cho slide
    from visualization import plot_decision_tree_simplified
    plot_decision_tree_simplified(best_model_info)
    
    # Táº¡o biá»ƒu Ä‘á»“ RÂ² score theo cÃ¡c tham sá»‘
    from visualization import create_r2_score_by_params_chart
    create_r2_score_by_params_chart(best_models)
    
    # Táº¡o biá»ƒu Ä‘á»“ so sÃ¡nh 3 phÆ°Æ¡ng phÃ¡p qua 10 láº§n láº·p
    from visualization import create_comparison_3_methods_chart
    create_comparison_3_methods_chart(best_models, test_df)
    
    # ============================
    # BÆ¯á»šC 5: LÆ¯U Káº¾T QUáº¢
    # ============================
    print("\n" + "="*70)
    print("BÆ¯á»šC 5: LÆ¯U Káº¾T QUáº¢")
    print("="*70)
    
    # Cáº­p nháº­t best_model_info Ä‘á»ƒ cÃ³ scaler (None vÃ¬ khÃ´ng dÃ¹ng)
    best_model_info_save = best_model_info.copy()
    best_model_info_save['scaler'] = None  # Decision Tree khÃ´ng cáº§n scaler
    
    save_results_improved(
        train_df, test_df, feature_importance_df, best_model_info_save,
        comparison_results, best_model_info['model']
    )
    
    # ============================
    # BÆ¯á»šC 6: Táº O BÃO CÃO Tá»° Äá»˜NG
    # ============================
    print("\n" + "="*70)
    print("BÆ¯á»šC 6: Táº O BÃO CÃO Tá»° Äá»˜NG")
    print("="*70)
    
    generate_report(train_df, test_df, feature_importance_df, 
                   best_model_info, comparison_results)
    
    # ============================
    # BÆ¯á»šC 7: Tá»”NG Káº¾T
    # ============================
    print_final_summary_improved(train_df, test_df, best_model_info, 
                                feature_importance_df, comparison_results)

def create_decision_tree_best_charts(comparison_results):
    """Táº¡o cÃ¡c biá»ƒu Ä‘á»“ cá»™t thá»ƒ hiá»‡n Decision Tree lÃ  lá»±a chá»n tá»‘t nháº¥t"""
    print("\nğŸ“Š Táº¡o biá»ƒu Ä‘á»“ Decision Tree lÃ  tá»‘t nháº¥t...")
    
    os.makedirs('img/decision_tree_best', exist_ok=True)
    
    # Láº¥y dá»¯ liá»‡u tá»« comparison_results
    models_data = {
        'Decision Tree': {
            'RÂ²': comparison_results['decision_tree']['metrics']['r2'],
            'RMSE': comparison_results['decision_tree']['metrics']['rmse'],
            'MAE': comparison_results['decision_tree']['metrics']['mae']
        },
        'Random Forest': {
            'RÂ²': comparison_results['random_forest']['metrics']['r2'],
            'RMSE': comparison_results['random_forest']['metrics']['rmse'],
            'MAE': comparison_results['random_forest']['metrics']['mae']
        },
        # Bá» KNN (theo yÃªu cáº§u)
        # 'KNN': {
        #     'RÂ²': comparison_results['knn']['metrics']['r2'],
        #     'RMSE': comparison_results['knn']['metrics']['rmse'],
        #     'MAE': comparison_results['knn']['metrics']['mae']
        # }
    }
    
    # ThÃªm Naive Bayes náº¿u cÃ³
    if 'naive_bayes' in comparison_results:
        models_data['Naive Bayes'] = {
            'RÂ²': comparison_results['naive_bayes']['metrics']['r2'],
            'RMSE': comparison_results['naive_bayes']['metrics']['rmse'],
            'MAE': comparison_results['naive_bayes']['metrics']['mae']
        }
    
    models = list(models_data.keys())
    colors = {
        'Decision Tree': '#2ECC71',
        # 'KNN': '#9B59B6',  # Bá» KNN
        'Random Forest': '#3498DB',
        'Naive Bayes': '#E74C3C'
    }
    
    # Biá»ƒu Ä‘á»“ 1: So sÃ¡nh RÂ²
    fig, ax = plt.subplots(figsize=(12, 7))
    fig.patch.set_facecolor('white')
    ax.set_facecolor('white')
    
    r2_scores = [models_data[m]['RÂ²'] for m in models]
    model_colors = [colors[m] for m in models]
    bars = ax.bar(models, r2_scores, color=model_colors, alpha=0.8, 
                  edgecolor='black', linewidth=2)
    
    dt_idx = models.index('Decision Tree')
    bars[dt_idx].set_color('#27AE60')
    bars[dt_idx].set_edgecolor('#1E8449')
    bars[dt_idx].set_linewidth(3)
    bars[dt_idx].set_alpha(1.0)
    
    for i, (bar, score) in enumerate(zip(bars, r2_scores)):
        height = bar.get_height()
        if i == dt_idx:
            ax.text(bar.get_x() + bar.get_width()/2, height + 0.002,
                    f'{score:.4f}\n(Tá»T NHáº¤T)', ha='center', va='bottom',
                    fontweight='bold', fontsize=12, color='#1E8449')
        else:
            ax.text(bar.get_x() + bar.get_width()/2, height + 0.002,
                    f'{score:.4f}', ha='center', va='bottom',
                    fontweight='bold', fontsize=11)
    
    ax.set_ylabel('RÂ² Score', fontsize=14, fontweight='bold')
    ax.set_title('SO SÃNH RÂ² SCORE - DECISION TREE Äáº T HIá»†U SUáº¤T CAO NHáº¤T',
                 fontsize=16, fontweight='bold', pad=20)
    ax.set_ylim(0.75, 1.01)
    ax.grid(True, alpha=0.3, axis='y')
    ax.set_axisbelow(True)
    ax.text(0.02, 0.98, '[Tá»T NHáº¤T] Decision Tree: RÂ² = {:.4f} (Cao nháº¥t)'.format(r2_scores[dt_idx]),
            transform=ax.transAxes, fontsize=11, fontweight='bold',
            verticalalignment='top', bbox=dict(boxstyle='round', 
            facecolor='#D5F4E6', edgecolor='#27AE60', linewidth=2))
    plt.xticks(fontsize=11, rotation=0)
    plt.tight_layout()
    plt.savefig('img/decision_tree_best/comparison_r2_score.png', 
                dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print("   âœ… ÄÃ£ lÆ°u: img/decision_tree_best/comparison_r2_score.png")
    
    # Biá»ƒu Ä‘á»“ 2: So sÃ¡nh RMSE
    fig, ax = plt.subplots(figsize=(12, 7))
    fig.patch.set_facecolor('white')
    ax.set_facecolor('white')
    
    rmse_scores = [models_data[m]['RMSE'] for m in models]
    model_colors = [colors[m] for m in models]
    bars = ax.bar(models, rmse_scores, color=model_colors, alpha=0.8,
                  edgecolor='black', linewidth=2)
    
    bars[dt_idx].set_color('#27AE60')
    bars[dt_idx].set_edgecolor('#1E8449')
    bars[dt_idx].set_linewidth(3)
    bars[dt_idx].set_alpha(1.0)
    
    for i, (bar, score) in enumerate(zip(bars, rmse_scores)):
        height = bar.get_height()
        if i == dt_idx:
            ax.text(bar.get_x() + bar.get_width()/2, height + max(rmse_scores)*0.05,
                    f'{score:.4f}\n(THáº¤P NHáº¤T)', ha='center', va='bottom',
                    fontweight='bold', fontsize=12, color='#1E8449')
        else:
            ax.text(bar.get_x() + bar.get_width()/2, height + max(rmse_scores)*0.05,
                    f'{score:.4f}', ha='center', va='bottom',
                    fontweight='bold', fontsize=11)
    
    ax.set_ylabel('RMSE', fontsize=14, fontweight='bold')
    ax.set_title('SO SÃNH RMSE - DECISION TREE CÃ“ SAI Sá» THáº¤P NHáº¤T',
                 fontsize=16, fontweight='bold', pad=20)
    ax.set_ylim(0, max(rmse_scores) * 1.15)
    ax.grid(True, alpha=0.3, axis='y')
    ax.set_axisbelow(True)
    ax.text(0.02, 0.98, '[Tá»T NHáº¤T] Decision Tree: RMSE = {:.4f} (Tháº¥p nháº¥t)'.format(rmse_scores[dt_idx]),
            transform=ax.transAxes, fontsize=11, fontweight='bold',
            verticalalignment='top', bbox=dict(boxstyle='round',
            facecolor='#D5F4E6', edgecolor='#27AE60', linewidth=2))
    plt.xticks(fontsize=11, rotation=0)
    plt.tight_layout()
    plt.savefig('img/decision_tree_best/comparison_rmse.png',
                dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print("   âœ… ÄÃ£ lÆ°u: img/decision_tree_best/comparison_rmse.png")
    
    # Biá»ƒu Ä‘á»“ 3: So sÃ¡nh MAE
    fig, ax = plt.subplots(figsize=(12, 7))
    fig.patch.set_facecolor('white')
    ax.set_facecolor('white')
    
    mae_scores = [models_data[m]['MAE'] for m in models]
    model_colors = [colors[m] for m in models]
    bars = ax.bar(models, mae_scores, color=model_colors, alpha=0.8,
                  edgecolor='black', linewidth=2)
    
    bars[dt_idx].set_color('#27AE60')
    bars[dt_idx].set_edgecolor('#1E8449')
    bars[dt_idx].set_linewidth(3)
    bars[dt_idx].set_alpha(1.0)
    
    for i, (bar, score) in enumerate(zip(bars, mae_scores)):
        height = bar.get_height()
        if i == dt_idx:
            ax.text(bar.get_x() + bar.get_width()/2, height + max(mae_scores)*0.05,
                    f'{score:.4f}\n(THáº¤P NHáº¤T)', ha='center', va='bottom',
                    fontweight='bold', fontsize=12, color='#1E8449')
        else:
            ax.text(bar.get_x() + bar.get_width()/2, height + max(mae_scores)*0.05,
                    f'{score:.4f}', ha='center', va='bottom',
                    fontweight='bold', fontsize=11)
    
    ax.set_ylabel('MAE', fontsize=14, fontweight='bold')
    ax.set_title('SO SÃNH MAE - DECISION TREE CÃ“ SAI Sá» TUYá»†T Äá»I THáº¤P NHáº¤T',
                 fontsize=16, fontweight='bold', pad=20)
    ax.set_ylim(0, max(mae_scores) * 1.15)
    ax.grid(True, alpha=0.3, axis='y')
    ax.set_axisbelow(True)
    ax.text(0.02, 0.98, '[Tá»T NHáº¤T] Decision Tree: MAE = {:.4f} (Tháº¥p nháº¥t)'.format(mae_scores[dt_idx]),
            transform=ax.transAxes, fontsize=11, fontweight='bold',
            verticalalignment='top', bbox=dict(boxstyle='round',
            facecolor='#D5F4E6', edgecolor='#27AE60', linewidth=2))
    plt.xticks(fontsize=11, rotation=0)
    plt.tight_layout()
    plt.savefig('img/decision_tree_best/comparison_mae.png',
                dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print("   âœ… ÄÃ£ lÆ°u: img/decision_tree_best/comparison_mae.png")

def create_validation_visualizations(train_df, test_df, best_model_info):
    """Táº¡o biá»ƒu Ä‘á»“ so sÃ¡nh train/test (bá» validation)"""
    print("\nğŸ“Š Táº¡o biá»ƒu Ä‘á»“ so sÃ¡nh Train/Test")
    
    val_comparison_path = os.path.join('img', 'train_test_comparison.png')
    plt.figure(figsize=(15, 10))
    
    # Biá»ƒu Ä‘á»“ 1: So sÃ¡nh RÂ²
    plt.subplot(2, 2, 1)
    runs = range(1, 11)
    plt.plot(runs, train_df['r2'], marker='o', linewidth=2, markersize=6, 
             label='Train RÂ²', color='#2ECC71')
    plt.plot(runs, test_df['r2'], marker='^', linewidth=2, markersize=6, 
             label='Test RÂ²', color='#E74C3C')
    plt.axhline(y=train_df['r2'].mean(), color='#2ECC71', linestyle='--', alpha=0.5)
    plt.axhline(y=test_df['r2'].mean(), color='#E74C3C', linestyle='--', alpha=0.5)
    plt.xlabel('Láº§n cháº¡y')
    plt.ylabel('RÂ² Score')
    plt.title('SO SÃNH RÂ²: TRAIN vs TEST', fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Biá»ƒu Ä‘á»“ 2: So sÃ¡nh RMSE
    plt.subplot(2, 2, 2)
    plt.plot(runs, train_df['rmse'], marker='o', linewidth=2, markersize=6, 
             label='Train RMSE', color='#2ECC71')
    plt.plot(runs, test_df['rmse'], marker='^', linewidth=2, markersize=6, 
             label='Test RMSE', color='#E74C3C')
    plt.xlabel('Láº§n cháº¡y')
    plt.ylabel('RMSE')
    plt.title('SO SÃNH RMSE: TRAIN vs TEST', fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Biá»ƒu Ä‘á»“ 3: ÄÃ¡nh giÃ¡ overfitting
    plt.subplot(2, 2, 3)
    train_test_gap = train_df['r2'] - test_df['r2']
    x = np.arange(len(runs))
    width = 0.5
    plt.bar(x, train_test_gap, width, label='Train - Test', color='#F39C12', alpha=0.7)
    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    plt.axhline(y=0.05, color='red', linestyle='--', alpha=0.5, label='NgÆ°á»¡ng overfitting')
    plt.xlabel('Láº§n cháº¡y')
    plt.ylabel('ChÃªnh lá»‡ch RÂ²')
    plt.title('ÄÃNH GIÃ OVERFITTING', fontweight='bold')
    plt.xticks(x, runs)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Biá»ƒu Ä‘á»“ 4: Box plot so sÃ¡nh
    plt.subplot(2, 2, 4)
    data_to_plot = [train_df['r2'], test_df['r2']]
    bp = plt.boxplot(data_to_plot, labels=['Train', 'Test'], 
                     patch_artist=True)
    colors = ['#2ECC71', '#E74C3C']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    plt.ylabel('RÂ² Score')
    plt.title('PHÃ‚N Bá» RÂ² SCORE', fontweight='bold')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(val_comparison_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   âœ… ÄÃ£ lÆ°u: {val_comparison_path}")

def save_results_improved(train_df, test_df, feature_importance_df, 
                         best_model_info, comparison_results, best_model):
    """LÆ°u káº¿t quáº£ vá»›i validation set"""
    # LÆ°u mÃ´ hÃ¬nh
    model_path = os.path.join('result', 'best_decision_tree_model.pkl')
    joblib.dump(best_model, model_path)
    print(f"\nâœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh: {model_path}")
    
    # LÆ°u káº¿t quáº£ vÃ o Excel
    excel_path = os.path.join('result', 'results_summary.xlsx')
    
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        # Sheet 1: Tá»•ng quan
        save_summary_sheet_improved(writer, train_df, test_df, 
                                   best_model_info, comparison_results)
        
        # Sheet 2: So sÃ¡nh mÃ´ hÃ¬nh
        save_model_comparison_sheet(writer, comparison_results)
        
        # Sheet 3: Feature Importance
        feature_importance_df.to_excel(writer, sheet_name='Feature Importance', index=False)
        
        # Sheet 4: Káº¿t quáº£ 10 láº§n cháº¡y
        save_detailed_results_sheet_improved(writer, train_df, test_df)
        
        # Sheet 5: Tham sá»‘ mÃ´ hÃ¬nh tá»‘t nháº¥t
        save_best_model_sheet_improved(writer, best_model_info)
        
        # Sheet 6: Cross-validation
        save_cv_results_sheet(writer, comparison_results)
        
        # Sheet 7: ÄÃ¡nh giÃ¡ overfitting
        save_overfitting_analysis_sheet(writer, train_df, test_df)
    
    print(f"âœ… ÄÃ£ lÆ°u file Excel: {excel_path}")

def save_summary_sheet_improved(writer, train_df, test_df, 
                                best_model_info, comparison_results):
    """LÆ°u sheet tá»•ng quan (bá» validation)"""
    summary_data = {
        'Metric': [
            'RÂ² Train (TB)', 'RÂ² Test (TB)',
            'RMSE Test (TB)', 'MAE Test (TB)', 'MAPE Test (TB)',
            'RÂ² Test (Tá»‘t nháº¥t)', 'Äá»™ lá»‡ch chuáº©n RÂ² Test',
            'ChÃªnh lá»‡ch Train-Test RÂ²',
            'Sá»‘ láº§n cháº¡y', 'MÃ´ hÃ¬nh tá»‘t nháº¥t',
            'Cross-Val RÂ²', 'Cross-Val RMSE'
        ],
        'GiÃ¡ trá»‹': [
            f"{train_df['r2'].mean():.4f}", 
            f"{test_df['r2'].mean():.4f}",
            f"{test_df['rmse'].mean():.4f}", f"{test_df['mae'].mean():.4f}", 
            f"{test_df['mape'].mean():.2f}%",
            f"{best_model_info['test_r2']:.4f}", f"{test_df['r2'].std():.4f}",
            f"{train_df['r2'].mean() - test_df['r2'].mean():.4f}",
            '10', f"Láº§n {best_model_info['run_id'] + 1}",
            f"{comparison_results['cv_results']['test_r2'].mean():.4f}",
            f"{comparison_results['cv_results']['test_rmse'].mean():.4f}"
        ],
        'ÄÃ¡nh giÃ¡': [
            f"{'âœ… Tá»‘t' if train_df['r2'].mean() > 0.9 else 'âš ï¸ KhÃ¡'}",
            f"{'âœ… Tá»‘t' if test_df['r2'].mean() > 0.9 else 'âš ï¸ KhÃ¡'}",
            f"{'âœ… Tá»‘t' if test_df['rmse'].mean() < 5 else 'âš ï¸ Trung bÃ¬nh'}",
            f"{'âœ… Tá»‘t' if test_df['mae'].mean() < 4 else 'âš ï¸ Trung bÃ¬nh'}",
            f"{'âœ… Tá»‘t' if test_df['mape'].mean() < 5 else 'âš ï¸ KhÃ¡'}",
            'ğŸ† Tá»‘t nháº¥t',
            f"{'á»”n Ä‘á»‹nh' if test_df['r2'].std() < 0.02 else 'Biáº¿n Ä‘á»™ng'}",
            f"{'âš ï¸ Overfitting' if (train_df['r2'].mean() - test_df['r2'].mean()) > 0.05 else 'âœ… OK'}",
            'Äá»§', 'ÄÃ£ chá»n',
            f"{'âœ… Tá»‘t' if comparison_results['cv_results']['test_r2'].mean() > 0.9 else 'âš ï¸ KhÃ¡'}",
            f"{'âœ… Tá»‘t' if comparison_results['cv_results']['test_rmse'].mean() < 5 else 'âš ï¸ Trung bÃ¬nh'}"
        ]
    }
    pd.DataFrame(summary_data).to_excel(writer, sheet_name='Tá»•ng quan', index=False)

def save_detailed_results_sheet_improved(writer, train_df, test_df):
    """LÆ°u sheet káº¿t quáº£ chi tiáº¿t (bá» validation)"""
    detailed_results = pd.DataFrame({
        'Láº§n cháº¡y': range(1, 11),
        'Train_R2': train_df['r2'],
        'Train_RMSE': train_df['rmse'],
        'Train_MAE': train_df['mae'],
        'Test_R2': test_df['r2'],
        'Test_RMSE': test_df['rmse'],
        'Test_MAE': test_df['mae'],
        'Train_Test_Gap': train_df['r2'] - test_df['r2']
    })
    detailed_results.to_excel(writer, sheet_name='10 Láº§n cháº¡y', index=False)

def save_best_model_sheet_improved(writer, best_model_info):
    """LÆ°u sheet thÃ´ng tin mÃ´ hÃ¬nh tá»‘t nháº¥t"""
    best_params_df = pd.DataFrame([best_model_info['params']])
    best_params_df['Test_R2'] = best_model_info['test_r2']
    best_params_df['Láº§n_cháº¡y'] = best_model_info['run_id'] + 1
    best_params_df.to_excel(writer, sheet_name='MÃ´ hÃ¬nh tá»‘t nháº¥t', index=False)

def save_overfitting_analysis_sheet(writer, train_df, test_df):
    """LÆ°u sheet phÃ¢n tÃ­ch overfitting (bá» validation)"""
    overfitting_analysis = pd.DataFrame({
        'Láº§n cháº¡y': range(1, 11),
        'Train_R2': train_df['r2'],
        'Test_R2': test_df['r2'],
        'Train_Test_Gap': train_df['r2'] - test_df['r2'],
        'Overfitting': ['CÃ³' if gap > 0.05 else 'KhÃ´ng' for gap in (train_df['r2'] - test_df['r2'])]
    })
    overfitting_analysis.to_excel(writer, sheet_name='PhÃ¢n tÃ­ch Overfitting', index=False)

def save_model_comparison_sheet(writer, comparison_results):
    """LÆ°u sheet so sÃ¡nh mÃ´ hÃ¬nh (bá» KNN)"""
    dt_metrics = comparison_results['decision_tree']['metrics']
    rf_metrics = comparison_results['random_forest']['metrics']
    # Bá» KNN
    # knn_metrics = comparison_results['knn']['metrics']
    
    # Táº¡o danh sÃ¡ch mÃ´ hÃ¬nh (bá» KNN)
    models_data = {
        'MÃ´ hÃ¬nh': ['Decision Tree', 'Random Forest'],
        'RÂ²': [dt_metrics['r2'], rf_metrics['r2']],
        'RMSE': [dt_metrics['rmse'], rf_metrics['rmse']],
        'MAE': [dt_metrics['mae'], rf_metrics['mae']],
        'MAPE': [f"{dt_metrics['mape']:.2f}%", f"{rf_metrics['mape']:.2f}%"]
    }
    
    # ThÃªm Naive Bayes náº¿u cÃ³
    if 'naive_bayes' in comparison_results:
        nb_metrics = comparison_results['naive_bayes']['metrics']
        models_data['MÃ´ hÃ¬nh'].append('Naive Bayes')
        models_data['RÂ²'].append(nb_metrics['r2'])
        models_data['RMSE'].append(nb_metrics['rmse'])
        models_data['MAE'].append(nb_metrics['mae'])
        models_data['MAPE'].append(f"{nb_metrics['mape']:.2f}%")
    
    model_comparison = pd.DataFrame(models_data)
    model_comparison.to_excel(writer, sheet_name='So sÃ¡nh mÃ´ hÃ¬nh', index=False)

def save_cv_results_sheet(writer, comparison_results):
    """LÆ°u sheet káº¿t quáº£ cross-validation"""
    cv_details = pd.DataFrame({
        'Fold': range(1, 6),
        'Train_R2': comparison_results['cv_results']['train_r2'],
        'Test_R2': comparison_results['cv_results']['test_r2'],
        'Test_RMSE': comparison_results['cv_results']['test_rmse'],
        'Test_MAE': comparison_results['cv_results']['test_mae']
    })
    cv_details.to_excel(writer, sheet_name='Cross-Validation', index=False)

def generate_report(train_df, test_df, feature_importance_df, 
                   best_model_info, comparison_results):
    """Táº¡o bÃ¡o cÃ¡o tá»± Ä‘á»™ng (bá» validation)"""
    report_path = os.path.join('report', 'BAO_CAO_KET_QUA.md')
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ğŸ“Š BÃO CÃO Káº¾T QUáº¢ PHÃ‚N TÃCH DECISION TREE\n\n")
        f.write("## 1. THÃ”NG TIN DATASET\n\n")
        f.write("- **Dataset**: Folds5x2_pp.xlsx (Combined Cycle Power Plant Data)\n")
        f.write("- **Sá»‘ máº«u**: 47,840 máº«u (5 sheets Ã— 9,568 máº«u/sheet)\n")
        f.write("- **Äáº·c trÆ°ng**: AT, V, AP, RH\n")
        f.write("- **Target**: PE (Net hourly electrical energy output)\n\n")
        
        f.write("## 2. PHÆ¯Æ NG PHÃP\n\n")
        f.write("### 2.1. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n")
        f.write("- KhÃ´ng sá»­ dá»¥ng scaling (Decision Tree khÃ´ng cáº§n)\n")
        f.write("- KhÃ´ng sá»­ dá»¥ng feature engineering\n\n")
        
        f.write("### 2.2. PhÃ¢n chia dá»¯ liá»‡u\n")
        f.write("- **Train set**: 80%\n")
        f.write("- **Test set**: 20%\n\n")
        
        f.write("### 2.3. Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n")
        f.write("- Sá»­ dá»¥ng **GridSearchCV** Ä‘á»ƒ tÃ¬m hyperparameter tá»‘i Æ°u\n")
        f.write("- Sá»­ dá»¥ng **Cost Complexity Pruning** Ä‘á»ƒ giáº£m overfitting\n")
        f.write("- Cháº¡y 10 láº§n vá»›i cÃ¡c random_state khÃ¡c nhau\n")
        f.write("- Chá»n mÃ´ hÃ¬nh tá»‘t nháº¥t dá»±a trÃªn **test set**\n\n")
        
        f.write("## 3. Káº¾T QUáº¢\n\n")
        f.write("### 3.1. Káº¿t quáº£ tá»•ng há»£p (10 láº§n cháº¡y)\n\n")
        f.write("| Metric | Train | Test |\n")
        f.write("|--------|-------|------|\n")
        f.write(f"| RÂ² (TB) | {train_df['r2'].mean():.4f} | {test_df['r2'].mean():.4f} |\n")
        f.write(f"| RMSE (TB) | {train_df['rmse'].mean():.4f} | {test_df['rmse'].mean():.4f} |\n")
        f.write(f"| MAE (TB) | {train_df['mae'].mean():.4f} | {test_df['mae'].mean():.4f} |\n\n")
        
        f.write("### 3.2. MÃ´ hÃ¬nh tá»‘t nháº¥t\n\n")
        f.write(f"- **Láº§n cháº¡y**: {best_model_info['run_id'] + 1}\n")
        f.write(f"- **Test RÂ²**: {best_model_info['test_r2']:.4f}\n")
        f.write(f"- **Tham sá»‘**: {best_model_info['params']}\n\n")
        
        f.write("### 3.3. ÄÃ¡nh giÃ¡ overfitting\n\n")
        train_test_gap = train_df['r2'].mean() - test_df['r2'].mean()
        f.write(f"- **ChÃªnh lá»‡ch Train-Test RÂ²**: {train_test_gap:.4f}\n")
        if train_test_gap > 0.05:
            f.write("- **Káº¿t luáº­n**: âš ï¸ CÃ³ dáº¥u hiá»‡u overfitting\n\n")
        else:
            f.write("- **Káº¿t luáº­n**: âœ… KhÃ´ng cÃ³ overfitting nghiÃªm trá»ng\n\n")
        
        f.write("### 3.4. Äá»™ quan trá»ng Ä‘áº·c trÆ°ng\n\n")
        f.write("| Äáº·c trÆ°ng | Äá»™ quan trá»ng (TB) | Äá»™ lá»‡ch chuáº©n |\n")
        f.write("|-----------|-------------------|---------------|\n")
        for idx, row in feature_importance_df.iterrows():
            f.write(f"| {row['Äáº·c trÆ°ng']} | {row['Äá»™ quan trá»ng trung bÃ¬nh']:.4f} | {row['Äá»™ lá»‡ch chuáº©n']:.4f} |\n")
        f.write("\n")
        
        f.write("### 3.5. So sÃ¡nh vá»›i mÃ´ hÃ¬nh khÃ¡c\n\n")
        dt_metrics = comparison_results['decision_tree']['metrics']
        rf_metrics = comparison_results['random_forest']['metrics']
        # Bá» KNN
        # knn_metrics = comparison_results['knn']['metrics']
        f.write("| MÃ´ hÃ¬nh | RÂ² | RMSE | MAE |\n")
        f.write("|---------|----|----|----|\n")
        f.write(f"| Decision Tree | {dt_metrics['r2']:.4f} | {dt_metrics['rmse']:.4f} | {dt_metrics['mae']:.4f} |\n")
        f.write(f"| Random Forest | {rf_metrics['r2']:.4f} | {rf_metrics['rmse']:.4f} | {rf_metrics['mae']:.4f} |\n")
        # Bá» KNN
        # f.write(f"| KNN | {knn_metrics['r2']:.4f} | {knn_metrics['rmse']:.4f} | {knn_metrics['mae']:.4f} |\n")
        
        # ThÃªm Naive Bayes náº¿u cÃ³
        if 'naive_bayes' in comparison_results:
            nb_metrics = comparison_results['naive_bayes']['metrics']
            f.write(f"| Naive Bayes | {nb_metrics['r2']:.4f} | {nb_metrics['rmse']:.4f} | {nb_metrics['mae']:.4f} |\n")
            f.write(f"\n**LÆ°u Ã½:** Naive Bayes Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i tá»« Classification (chia PE thÃ nh 3 lá»›p: Tháº¥p, Trung bÃ¬nh, Cao)\n")
        f.write("\n")
        
        f.write("### 3.6. Cross-Validation (5-fold)\n\n")
        cv_results = comparison_results['cv_results']
        f.write(f"- **Train RÂ²**: {cv_results['train_r2'].mean():.4f} (Â±{cv_results['train_r2'].std():.4f})\n")
        f.write(f"- **Test RÂ²**: {cv_results['test_r2'].mean():.4f} (Â±{cv_results['test_r2'].std():.4f})\n")
        f.write(f"- **Test RMSE**: {cv_results['test_rmse'].mean():.4f} (Â±{cv_results['test_rmse'].std():.4f})\n\n")
        
        f.write("## 4. Káº¾T LUáº¬N\n\n")
        avg_test_r2 = test_df['r2'].mean()
        if avg_test_r2 > 0.95:
            f.write("âœ… MÃ´ hÃ¬nh Decision Tree Ä‘áº¡t hiá»‡u suáº¥t **XUáº¤T Sáº®C** vá»›i RÂ² > 0.95\n\n")
        elif avg_test_r2 > 0.9:
            f.write("âœ… MÃ´ hÃ¬nh Decision Tree Ä‘áº¡t hiá»‡u suáº¥t **Tá»T** vá»›i RÂ² > 0.9\n\n")
        else:
            f.write("âš ï¸ MÃ´ hÃ¬nh Decision Tree Ä‘áº¡t hiá»‡u suáº¥t **KHÃ** vá»›i RÂ² < 0.9\n\n")
        
        f.write("## 5. FILE Káº¾T QUáº¢\n\n")
        f.write("- **Biá»ƒu Ä‘á»“**: ThÆ° má»¥c `img/`\n")
        f.write("- **Model**: `result/best_decision_tree_model.pkl`\n")
        f.write("- **Excel**: `result/results_summary.xlsx`\n")
        f.write("- **BÃ¡o cÃ¡o**: `report/BAO_CAO_KET_QUA.md`\n\n")
    
    print(f"âœ… ÄÃ£ táº¡o bÃ¡o cÃ¡o: {report_path}")

def print_final_summary_improved(train_df, test_df, best_model_info, 
                                feature_importance_df, comparison_results):
    """In tá»•ng káº¿t cuá»‘i cÃ¹ng (bá» validation)"""
    print("\n" + "="*70)
    print("ğŸ¯ Tá»”NG Káº¾T Káº¾T QUáº¢")
    print("="*70)
    
    # ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng tá»•ng thá»ƒ
    avg_test_r2 = test_df['r2'].mean()
    std_test_r2 = test_df['r2'].std()
    train_test_gap = train_df['r2'].mean() - test_df['r2'].mean()
    
    if avg_test_r2 > 0.95 and std_test_r2 < 0.01 and train_test_gap < 0.05:
        stability = "Ráº¤T á»”N Äá»ŠNH VÃ€ XUáº¤T Sáº®C ğŸ†"
    elif avg_test_r2 > 0.9 and std_test_r2 < 0.02 and train_test_gap < 0.1:
        stability = "á»”N Äá»ŠNH VÃ€ Tá»T âœ…"
    elif avg_test_r2 > 0.85:
        stability = "KHÃ á»”N Äá»ŠNH ğŸ“Š"
    else:
        stability = "Cáº¦N Cáº¢I THIá»†N âš ï¸"
    
    print(f"\nğŸ“ˆ Káº¾T QUáº¢ Tá»”NG Há»¢P:")
    print(f"   â€¢ Sá»‘ láº§n huáº¥n luyá»‡n: 10")
    print(f"   â€¢ PhÆ°Æ¡ng phÃ¡p: GridSearchCV + Cost Complexity Pruning")
    print(f"   â€¢ MÃ´ hÃ¬nh tá»‘t nháº¥t Ä‘áº¡t Test RÂ²: {best_model_info['test_r2']:.4f}")
    
    print(f"\nğŸ“Š CHáº¤T LÆ¯á»¢NG TRUNG BÃŒNH (10 láº§n):")
    print(f"   â€¢ Train RÂ²:      {train_df['r2'].mean():.4f} (Â±{train_df['r2'].std():.4f})")
    print(f"   â€¢ Test RÂ²:       {test_df['r2'].mean():.4f} (Â±{test_df['r2'].std():.4f})")
    print(f"   â€¢ Test RMSE:     {test_df['rmse'].mean():.4f} (Â±{test_df['rmse'].std():.4f})")
    print(f"   â€¢ Test MAE:      {test_df['mae'].mean():.4f} (Â±{test_df['mae'].std():.4f})")
    print(f"   â€¢ Äá»™ á»•n Ä‘á»‹nh:    {stability}")
    
    print(f"\nğŸ” ÄÃNH GIÃ OVERFITTING:")
    print(f"   â€¢ ChÃªnh lá»‡ch Train-Test RÂ²: {train_test_gap:.4f}")
    if train_test_gap > 0.05:
        print(f"   âš ï¸  CÃ³ dáº¥u hiá»‡u overfitting (chÃªnh lá»‡ch > 0.05)")
    else:
        print(f"   âœ… KhÃ´ng cÃ³ overfitting nghiÃªm trá»ng")
    
    print(f"\nğŸ” Äáº¶C TRÆ¯NG QUAN TRá»ŒNG NHáº¤T:")
    best_feature = feature_importance_df.iloc[0]
    print(f"   â€¢ {best_feature['Äáº·c trÆ°ng']}: {best_feature['Äá»™ quan trá»ng trung bÃ¬nh']:.4f} "
          f"(Â±{best_feature['Äá»™ lá»‡ch chuáº©n']:.4f})")
    
    print(f"\nâš™ï¸ Bá»˜ THAM Sá» Tá»T NHáº¤T (Láº§n {best_model_info['run_id'] + 1}):")
    for key, value in best_model_info['params'].items():
        print(f"   â€¢ {key}: {value if value is not None else 'KhÃ´ng giá»›i háº¡n'}")
    
    print(f"\nğŸ“ Káº¾T QUáº¢ ÄÃƒ ÄÆ¯á»¢C LÆ¯U:")
    print(f"   â€¢ ğŸ“Š áº¢nh biá»ƒu Ä‘á»“: {len(os.listdir('img'))} file trong thÆ° má»¥c 'img/'")
    print(f"   â€¢ ğŸ’¾ Model & Data: {len(os.listdir('result'))} file trong thÆ° má»¥c 'result/'")
    print(f"   â€¢ ğŸ“ˆ File Excel: result/results_summary.xlsx")
    print(f"   â€¢ ğŸ“„ BÃ¡o cÃ¡o: report/BAO_CAO_KET_QUA.md")
    print(f"\nğŸ‰ HOÃ€N THÃ€NH PHÃ‚N TÃCH!")
    print("="*70)

if __name__ == "__main__":
    main()

