"""
PHIÃŠN Báº¢N Cáº¢I THIá»†N Cá»¦A MODEL_TRAINER.PY

CÃ¡c cáº£i thiá»‡n chÃ­nh:
1. Loáº¡i bá» scaling khÃ´ng cáº§n thiáº¿t cho Decision Tree
2. Sá»­ dá»¥ng GridSearchCV Ä‘á»ƒ tÃ¬m hyperparameter tá»‘i Æ°u
3. Sá»­ dá»¥ng nested cross-validation Ä‘á»ƒ trÃ¡nh data leakage
4. ThÃªm cost complexity pruning
5. Cáº£i thiá»‡n quy trÃ¬nh Ä‘Ã¡nh giÃ¡
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, KFold
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import warnings
import sys
import random
warnings.filterwarnings('ignore')

def calculate_metrics(y_true, y_pred):
    """
    TÃ­nh cÃ¡c metrics Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh
    """
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mse)
    # Xá»­ lÃ½ trÆ°á»ng há»£p y_true = 0 Ä‘á»ƒ trÃ¡nh lá»—i chia cho 0
    mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true != 0, y_true, 1e-10))) * 100
    
    return {
        'mse': mse, 'rmse': rmse, 'mae': mae, 
        'r2': r2, 'mape': mape
    }

def find_optimal_ccp_alpha(X_train, y_train, X_test, y_test, base_params):
    """
    TÃ¬m giÃ¡ trá»‹ ccp_alpha tá»‘i Æ°u báº±ng cÃ¡ch thá»­ nghiá»‡m trÃªn test set
    (hoáº·c cÃ³ thá»ƒ dÃ¹ng cross-validation trÃªn train set)
    """
    # Táº¡o cÃ¢y vá»›i cÃ¡c giÃ¡ trá»‹ ccp_alpha khÃ¡c nhau
    ccp_alphas = np.logspace(-4, -1, 20)
    best_ccp_alpha = 0.0
    best_r2 = -np.inf
    
    # Sá»­ dá»¥ng cross-validation trÃªn train set Ä‘á»ƒ tÃ¬m ccp_alpha tá»‘t nháº¥t
    # (trÃ¡nh dÃ¹ng test set cho viá»‡c tuning)
    from sklearn.model_selection import cross_val_score
    for ccp_alpha in ccp_alphas:
        model = DecisionTreeRegressor(
            random_state=42,
            ccp_alpha=ccp_alpha,
            **base_params
        )
        # Sá»­ dá»¥ng 5-fold CV trÃªn train set
        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
        avg_r2 = scores.mean()
        
        if avg_r2 > best_r2:
            best_r2 = avg_r2
            best_ccp_alpha = ccp_alpha
    
    return best_ccp_alpha

def train_decision_trees_improved(X, y, n_runs=10, use_grid_search=True):
    """
    Huáº¥n luyá»‡n Decision Tree vá»›i phÆ°Æ¡ng phÃ¡p cáº£i thiá»‡n
    
    Parameters:
    - X: features (KHÃ”NG cáº§n scaling cho Decision Tree)
    - y: target
    - n_runs: sá»‘ láº§n cháº¡y Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ á»•n Ä‘á»‹nh
    - use_grid_search: cÃ³ sá»­ dá»¥ng GridSearchCV khÃ´ng
    
    Returns:
    - train_df: DataFrame chá»©a metrics táº­p train
    - test_df: DataFrame chá»©a metrics táº­p test  
    - feature_importance_df: DataFrame chá»©a Ä‘á»™ quan trá»ng Ä‘áº·c trÆ°ng
    - best_models: danh sÃ¡ch cÃ¡c mÃ´ hÃ¬nh tá»‘t nháº¥t
    - best_model_info: thÃ´ng tin mÃ´ hÃ¬nh tá»‘t nháº¥t
    """
    # Lists Ä‘á»ƒ lÆ°u káº¿t quáº£
    all_train_metrics = []
    all_test_metrics = []
    all_feature_importances = []
    best_models = []
    
    # Äá»‹nh nghÄ©a grid search parameters - tá»‘i Æ°u Ä‘á»ƒ cÃ¢n báº±ng giá»¯a cháº¥t lÆ°á»£ng vÃ  tá»‘c Ä‘á»™
    if use_grid_search:
        # Giáº£m sá»‘ lÆ°á»£ng tham sá»‘ Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™
        # Loáº¡i bá» cÃ¡c giÃ¡ trá»‹ dá»… gÃ¢y overfitting (min_samples_split=2, min_samples_leaf=1, max_features=None)
        # Má»Ÿ rá»™ng param_grid Ä‘á»ƒ cÃ³ nhiá»u lá»±a chá»n hÆ¡n, trÃ¡nh chá»n cÃ¹ng má»™t bá»™ tham sá»‘
        # Tá»•ng sá»‘ tá»• há»£p: 8 Ã— 5 Ã— 5 Ã— 3 = 600 tá»• há»£p
        # Vá»›i 5-fold CV: 600 Ã— 5 = 3,000 mÃ´ hÃ¬nh
        param_grid = {
            'max_depth': [5, 7, 9, 10, 12, 15, 18, 20],    # 8 giÃ¡ trá»‹ (thÃªm 9, 18)
            'min_samples_split': [5, 8, 10, 15, 20],       # 5 giÃ¡ trá»‹ (thÃªm 8)
            'min_samples_leaf': [2, 3, 4, 5, 10],          # 5 giÃ¡ trá»‹ (thÃªm 4)
            'max_features': ['sqrt', 'log2', None]         # 3 giÃ¡ trá»‹ (thÃªm láº¡i None Ä‘á»ƒ Ä‘a dáº¡ng)
        }
    else:
        # Fallback: sá»­ dá»¥ng cÃ¡c bá»™ tham sá»‘ vá»›i max_depth ngáº«u nhiÃªn
        # Loáº¡i bá» None Ä‘á»ƒ trÃ¡nh overfitting
        max_depth_options = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]  # Loáº¡i bá» None
        min_samples_split_options = [2, 5, 8, 10, 12, 15, 20, 25, 30]
        min_samples_leaf_options = [1, 2, 3, 4, 5, 6, 8, 10, 12, 15]
        max_features_options = ['sqrt', 'log2', None]
        
        # Táº¡o 10 bá»™ tham sá»‘ ngáº«u nhiÃªn
        random.seed(42)  # Äá»ƒ cÃ³ thá»ƒ reproduce
        param_sets = []
        for _ in range(10):
            param_sets.append({
                'max_depth': random.choice(max_depth_options),
                'min_samples_split': random.choice(min_samples_split_options),
                'min_samples_leaf': random.choice(min_samples_leaf_options),
                'max_features': random.choice(max_features_options)
            })
    
    print(f"\n{'='*60}")
    print(f"HUáº¤N LUYá»†N DECISION TREE (PHÆ¯Æ NG PHÃP Cáº¢I THIá»†N)")
    print(f"{'='*60}")
    print(f"â€¢ Sá»‘ láº§n cháº¡y: {n_runs}")
    print(f"â€¢ Sá»­ dá»¥ng GridSearchCV: {use_grid_search}")
    print(f"â€¢ KhÃ´ng sá»­ dá»¥ng scaling (Decision Tree khÃ´ng cáº§n)")
    print(f"â€¢ Sá»­ dá»¥ng train/test split (80/20)")
    
    for i in range(n_runs):
        print(f"\n{'='*60}")
        print(f"ğŸ”„ Láº¦N CHáº Y THá»¨ {i+1}/{n_runs}")
        print(f"{'='*60}")
        sys.stdout.flush()
        
        # PhÃ¢n chia train/test vá»›i random_state khÃ¡c nhau
        # 80% train, 20% test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42 + i, shuffle=True
        )
        
        print(f"   ğŸ“Š KÃ­ch thÆ°á»›c: Train={len(X_train)}, Test={len(X_test)}")
        sys.stdout.flush()
        
        # TÃ¬m hyperparameter tá»‘i Æ°u
        if use_grid_search:
            print(f"   ğŸ” Äang tÃ¬m hyperparameter tá»‘i Æ°u vá»›i GridSearchCV...")
            sys.stdout.flush()
            
            # Sá»­ dá»¥ng GridSearchCV vá»›i 5-fold CV trÃªn train set
            base_model = DecisionTreeRegressor(random_state=42 + i)
            grid_search = GridSearchCV(
                base_model,
                param_grid,
                cv=5,
                scoring='r2',
                n_jobs=-1,
                verbose=0
            )
            grid_search.fit(X_train, y_train)
            
            best_params = grid_search.best_params_
            print(f"   âœ“ Tham sá»‘ tá»‘t nháº¥t (CV score: {grid_search.best_score_:.4f}): {best_params}")
            sys.stdout.flush()
        else:
            # Sá»­ dá»¥ng bá»™ tham sá»‘ cá»‘ Ä‘á»‹nh
            best_params = param_sets[i % len(param_sets)]
            print(f"   ğŸ“ Tham sá»‘: {best_params}")
            sys.stdout.flush()
        
        # TÃ¬m ccp_alpha tá»‘i Æ°u trÃªn train set (dÃ¹ng cross-validation)
        # (chá»‰ náº¿u chÆ°a cÃ³ trong best_params tá»« GridSearchCV)
        if 'ccp_alpha' not in best_params:
            print(f"   ğŸ” Äang tÃ¬m ccp_alpha tá»‘i Æ°u (dÃ¹ng cross-validation trÃªn train set)...")
            sys.stdout.flush()
            best_ccp_alpha = find_optimal_ccp_alpha(
                X_train, y_train, X_test, y_test, best_params
            )
            
            if best_ccp_alpha > 0:
                best_params['ccp_alpha'] = best_ccp_alpha
                print(f"   âœ“ ccp_alpha tá»‘i Æ°u: {best_ccp_alpha:.6f}")
            else:
                print(f"   âœ“ KhÃ´ng cáº§n pruning (ccp_alpha = 0)")
            sys.stdout.flush()
        else:
            print(f"   âœ“ ccp_alpha tá»« GridSearchCV: {best_params['ccp_alpha']:.6f}")
            sys.stdout.flush()
        
        # Táº¡o vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh cuá»‘i cÃ¹ng vá»›i train data
        if isinstance(X_train, pd.DataFrame):
            X_train_final = X_train.values
        else:
            X_train_final = X_train
        
        if isinstance(y_train, pd.Series):
            y_train_final = y_train.values
        else:
            y_train_final = y_train
        
        dt_model = DecisionTreeRegressor(
            random_state=42 + i,
            **best_params
        )
        dt_model.fit(X_train_final, y_train_final)
        
        # Dá»± Ä‘oÃ¡n trÃªn cÃ¡c táº­p khÃ¡c nhau
        y_pred_train = dt_model.predict(X_train)
        y_pred_test = dt_model.predict(X_test)
        
        # TÃ­nh metrics
        train_metrics = calculate_metrics(y_train, y_pred_train)
        test_metrics = calculate_metrics(y_test, y_pred_test)
        
        # LÆ°u káº¿t quáº£
        all_train_metrics.append(train_metrics)
        all_test_metrics.append(test_metrics)
        all_feature_importances.append(dt_model.feature_importances_)
        
        best_models.append({
            'model': dt_model,
            'params': best_params,
            'test_r2': test_metrics['r2'],  # Chá»n dá»±a trÃªn test
            'run_id': i,
            'X_train': X_train, 'y_train': y_train,
            'X_test': X_test, 'y_test': y_test,
            'y_pred_test': y_pred_test,
        })
        
        print(f"   ğŸ“ˆ Káº¿t quáº£:")
        print(f"      â€¢ Train RÂ²: {train_metrics['r2']:.4f}")
        print(f"      â€¢ Test RÂ²:  {test_metrics['r2']:.4f}")
        print(f"      â€¢ Test RMSE: {test_metrics['rmse']:.4f}")
        sys.stdout.flush()
    
    # Táº¡o DataFrames tá»« káº¿t quáº£
    train_df = pd.DataFrame(all_train_metrics)
    test_df = pd.DataFrame(all_test_metrics)
    
    # TÃ­nh Ä‘á»™ quan trá»ng Ä‘áº·c trÆ°ng trung bÃ¬nh
    avg_feature_importance = np.mean(all_feature_importances, axis=0)
    
    # Láº¥y tÃªn feature
    if hasattr(X, 'columns'):
        feature_names = list(X.columns)
    elif isinstance(X, pd.DataFrame):
        feature_names = list(X.columns)
    else:
        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
    
    feature_importance_df = pd.DataFrame({
        'Äáº·c trÆ°ng': feature_names,
        'Äá»™ quan trá»ng trung bÃ¬nh': avg_feature_importance,
        'Äá»™ lá»‡ch chuáº©n': np.std(all_feature_importances, axis=0)
    }).sort_values('Äá»™ quan trá»ng trung bÃ¬nh', ascending=False)
    
    # Chá»n mÃ´ hÃ¬nh tá»‘t nháº¥t dá»±a trÃªn TEST score
    best_models.sort(key=lambda x: x['test_r2'], reverse=True)
    best_model_info = best_models[0]
    
    # In káº¿t quáº£ tá»•ng há»£p
    print_improved_summary(train_df, test_df, feature_importance_df)
    
    return train_df, test_df, feature_importance_df, best_models, best_model_info

def print_improved_summary(train_df, test_df, feature_importance_df):
    """In tá»•ng káº¿t káº¿t quáº£ vá»›i train/test split"""
    print("\n" + "="*60)
    print("PHÃ‚N TÃCH Tá»”NG Há»¢P (PHÆ¯Æ NG PHÃP Cáº¢I THIá»†N)")
    print("="*60)
    
    print("\nğŸ“Š THá»NG KÃŠ Táº¬P TRAIN (10 láº§n):")
    print(f"   RÂ²:     {train_df['r2'].mean():.4f} (Â±{train_df['r2'].std():.4f})")
    print(f"   RMSE:   {train_df['rmse'].mean():.4f} (Â±{train_df['rmse'].std():.4f})")
    print(f"   MAE:    {train_df['mae'].mean():.4f} (Â±{train_df['mae'].std():.4f})")
    
    print("\nğŸ“Š THá»NG KÃŠ Táº¬P TEST (10 láº§n):")
    print(f"   RÂ²:     {test_df['r2'].mean():.4f} (Â±{test_df['r2'].std():.4f})")
    print(f"   RMSE:   {test_df['rmse'].mean():.4f} (Â±{test_df['rmse'].std():.4f})")
    print(f"   MAE:    {test_df['mae'].mean():.4f} (Â±{test_df['mae'].std():.4f})")
    
    # ÄÃ¡nh giÃ¡ overfitting
    train_test_gap = train_df['r2'].mean() - test_df['r2'].mean()
    
    print(f"\nğŸ” ÄÃNH GIÃ OVERFITTING:")
    print(f"   ChÃªnh lá»‡ch Train-Test RÂ²: {train_test_gap:.4f}")
    if train_test_gap > 0.05:
        print(f"   âš ï¸  CÃ³ dáº¥u hiá»‡u overfitting (chÃªnh lá»‡ch > 0.05)")
    else:
        print(f"   âœ… KhÃ´ng cÃ³ overfitting nghiÃªm trá»ng")
    
    print("\nğŸ” Äá»˜ QUAN TRá»ŒNG Äáº¶C TRÆ¯NG TRUNG BÃŒNH:")
    for idx, row in feature_importance_df.iterrows():
        print(f"   âœ“ {row['Äáº·c trÆ°ng']}: {row['Äá»™ quan trá»ng trung bÃ¬nh']:.4f} (Â±{row['Äá»™ lá»‡ch chuáº©n']:.4f})")

