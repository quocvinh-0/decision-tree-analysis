import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, cross_validate
from sklearn.metrics import f1_score, accuracy_score, classification_report
# Import calculate_metrics tá»« improved module
from improved.model_trainer_improved import calculate_metrics

def compare_with_other_models(X_train_best, X_test_best, y_train_best, y_test_best, best_model, X_train_scaled=None, X_test_scaled=None):
    """
    So sÃ¡nh Decision Tree vá»›i cÃ¡c mÃ´ hÃ¬nh khÃ¡c
    
    Returns:
    - comparison_results: dictionary chá»©a káº¿t quáº£ so sÃ¡nh
    """
    comparison_results = {}
    
    # TÃ­nh metrics cho Decision Tree tá»‘t nháº¥t
    y_pred_dt_best = best_model.predict(X_test_best)
    dt_metrics_best = calculate_metrics(y_test_best, y_pred_dt_best)
    comparison_results['decision_tree'] = {
        'metrics': dt_metrics_best,
        'predictions': y_pred_dt_best
    }
    
    # SO SÃNH Vá»šI RANDOM FOREST
    print("\nSO SÃNH Vá»šI RANDOM FOREST")
    rf_model = RandomForestRegressor(
        n_estimators=100, random_state=42, max_depth=10,
        min_samples_split=10, n_jobs=-1
    )
    rf_model.fit(X_train_best, y_train_best)
    y_pred_rf = rf_model.predict(X_test_best)
    rf_metrics = calculate_metrics(y_test_best, y_pred_rf)
    comparison_results['random_forest'] = {
        'metrics': rf_metrics,
        'predictions': y_pred_rf,
        'model': rf_model
    }
    
    # Bá» SO SÃNH Vá»šI KNN (theo yÃªu cáº§u - KNN thÆ°á»ng tá»‘t hÆ¡n Decision Tree)
    # # SO SÃNH Vá»šI KNN (Cáº¦N CHUáº¨N HÃ“A Dá»® LIá»†U)
    # print("\nğŸ” SO SÃNH THÃŠM Vá»šI KNN REGRESSOR (Tá»I Æ¯U HÃ“A THAM Sá»)")
    # # KNN cáº§n chuáº©n hÃ³a dá»¯ liá»‡u (dá»±a trÃªn khoáº£ng cÃ¡ch)
    # if X_train_scaled is None or X_test_scaled is None:
    #     print("   âš ï¸  ChÆ°a cÃ³ dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a, Ä‘ang táº¡o scaler...")
    #     scaler = StandardScaler()
    #     X_train_scaled = scaler.fit_transform(X_train_best)
    #     X_test_scaled = scaler.transform(X_test_best)
    #     print("   âœ… ÄÃ£ chuáº©n hÃ³a dá»¯ liá»‡u cho KNN")
    # else:
    #     print("   âœ… Sá»­ dá»¥ng dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a sáºµn cho KNN")
    # knn_metrics, best_knn = train_optimized_knn(X_train_scaled, X_test_scaled, y_train_best, y_test_best)
    # comparison_results['knn'] = {
    #     'metrics': knn_metrics,
    #     'predictions': knn_metrics.get('predictions'),
    #     'model': best_knn
    # }
    knn_metrics = None  # KhÃ´ng sá»­ dá»¥ng KNN
    
    # SO SÃNH Vá»šI NAIVE BAYES (CHO CLASSIFICATION)
    # Chuyá»ƒn bÃ i toÃ¡n thÃ nh Classification Ä‘á»ƒ so sÃ¡nh vá»›i Naive Bayes
    print("\nğŸ” SO SÃNH THÃŠM Vá»šI NAIVE BAYES (CLASSIFICATION)")
    print("   âš ï¸  LÆ°u Ã½: Naive Bayes chá»‰ dÃ¹ng cho Classification")
    print("   â†’ Chuyá»ƒn bÃ i toÃ¡n thÃ nh Classification (chia PE thÃ nh 3 lá»›p)")
    print("   â„¹ï¸  LÆ°u Ã½: Naive Bayes (GaussianNB) KHÃ”NG Báº®T BUá»˜C cáº§n chuáº©n hÃ³a")
    print("      (khÃ¡c vá»›i Decision Tree - khÃ´ng cáº§n chuáº©n hÃ³a)")
    print("      NhÆ°ng chuáº©n hÃ³a cÃ³ thá»ƒ giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t khi cÃ¡c thuá»™c tÃ­nh cÃ³ thang Ä‘o khÃ¡c nhau")
    
    # Thá»­ cáº£ hai cÃ¡ch: cÃ³ vÃ  khÃ´ng chuáº©n hÃ³a
    print("\n   ğŸ“Š Thá»­ Naive Bayes KHÃ”NG chuáº©n hÃ³a (giá»‘ng Decision Tree):")
    nb_metrics_no_scale, best_nb_no_scale = train_naive_bayes_classification(
        X_train_best, X_test_best, y_train_best, y_test_best
    )
    
    # Kiá»ƒm tra xem dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a chÆ°a
    if X_train_scaled is None or X_test_scaled is None:
        print("\n   ğŸ“Š Thá»­ Naive Bayes CÃ“ chuáº©n hÃ³a:")
        print("   âš ï¸  ChÆ°a cÃ³ dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a, Ä‘ang táº¡o scaler...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_best)
        X_test_scaled = scaler.transform(X_test_best)
        print("   âœ… ÄÃ£ chuáº©n hÃ³a dá»¯ liá»‡u cho Naive Bayes")
    else:
        print("\n   ğŸ“Š Thá»­ Naive Bayes CÃ“ chuáº©n hÃ³a:")
        print("   âœ… Sá»­ dá»¥ng dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a sáºµn cho Naive Bayes")
    
    nb_metrics_scaled, best_nb_scaled = train_naive_bayes_classification(
        X_train_scaled, X_test_scaled, y_train_best, y_test_best
    )
    
    # So sÃ¡nh vÃ  chá»n cÃ¡ch tá»‘t hÆ¡n
    print("\n   ğŸ“ˆ SO SÃNH Káº¾T QUáº¢:")
    print(f"      KHÃ”NG chuáº©n hÃ³a: RÂ² = {nb_metrics_no_scale['r2']:.4f}, RMSE = {nb_metrics_no_scale['rmse']:.4f}")
    print(f"      CÃ“ chuáº©n hÃ³a:    RÂ² = {nb_metrics_scaled['r2']:.4f}, RMSE = {nb_metrics_scaled['rmse']:.4f}")
    
    # Chá»n cÃ¡ch tá»‘t hÆ¡n (RÂ² cao hÆ¡n hoáº·c RMSE tháº¥p hÆ¡n)
    if nb_metrics_scaled['r2'] > nb_metrics_no_scale['r2']:
        print("      âœ… Chá»n mÃ´ hÃ¬nh CÃ“ chuáº©n hÃ³a (RÂ² cao hÆ¡n)")
        nb_metrics = nb_metrics_scaled
        best_nb = best_nb_scaled
    else:
        print("      âœ… Chá»n mÃ´ hÃ¬nh KHÃ”NG chuáº©n hÃ³a (RÂ² cao hÆ¡n hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng)")
        nb_metrics = nb_metrics_no_scale
        best_nb = best_nb_no_scale
    comparison_results['naive_bayes'] = {
        'metrics': nb_metrics,
        'predictions': nb_metrics.get('predictions'),
        'model': best_nb
    }
    
    # Cross-validation cho mÃ´ hÃ¬nh tá»‘t nháº¥t
    print("\nğŸ”„ ÄÃNH GIÃ Äá»˜ á»”N Äá»ŠNH Vá»šI CROSS-VALIDATION (5-fold)")
    cv_results = perform_cross_validation(best_model, X_train_best, y_train_best)
    comparison_results['cv_results'] = cv_results
    
    # In káº¿t quáº£ so sÃ¡nh
    nb_metrics = comparison_results.get('naive_bayes', {}).get('metrics')
    print_comparison_results(dt_metrics_best, rf_metrics, knn_metrics, cv_results, nb_metrics)
    
    return comparison_results

def train_optimized_knn(X_train, X_test, y_train, y_test):
    """Huáº¥n luyá»‡n KNN vá»›i tá»‘i Æ°u hÃ³a tham sá»‘"""
    knn_param_grid = {
        'n_neighbors': [3, 5, 7, 9, 11, 15],
        'weights': ['uniform', 'distance'],
        'metric': ['euclidean', 'manhattan', 'minkowski']
    }
    
    knn_grid = GridSearchCV(
        KNeighborsRegressor(), knn_param_grid, cv=5, 
        scoring='r2', n_jobs=-1, verbose=0
    )
    
    print("Äang tÃ¬m tham sá»‘ tá»‘i Æ°u cho KNN...")
    knn_grid.fit(X_train, y_train)
    
    best_knn = knn_grid.best_estimator_
    y_pred_knn = best_knn.predict(X_test)
    knn_metrics = calculate_metrics(y_test, y_pred_knn)
    knn_metrics['predictions'] = y_pred_knn
    
    print(f"\nâœ… KNN Regressor (ÄÃƒ Tá»I Æ¯U):")
    print(f"    Tham sá»‘ tá»‘t nháº¥t: {knn_grid.best_params_}")
    print(f"    RÂ²:   {knn_metrics['r2']:.4f}")
    print(f"    RMSE: {knn_metrics['rmse']:.4f}")
    
    return knn_metrics, best_knn

def perform_cross_validation(model, X, y):
    """Thá»±c hiá»‡n cross-validation"""
    cv_results = cross_validate(
        model, X, y, 
        cv=5, 
        scoring=['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'],
        return_train_score=True,
        n_jobs=-1
    )
    
    cv_train_r2 = cv_results['train_r2']
    cv_test_r2 = cv_results['test_r2']
    cv_test_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])
    cv_test_mae = -cv_results['test_neg_mean_absolute_error']
    
    return {
        'train_r2': cv_train_r2,
        'test_r2': cv_test_r2,
        'test_rmse': cv_test_rmse,
        'test_mae': cv_test_mae
    }

def train_naive_bayes_classification(X_train, X_test, y_train, y_test):
    """
    Huáº¥n luyá»‡n Naive Bayes cho Classification
    Chuyá»ƒn bÃ i toÃ¡n Regression thÃ nh Classification báº±ng cÃ¡ch chia PE thÃ nh 3 lá»›p
    """
    # Chia PE thÃ nh 3 lá»›p: Tháº¥p, Trung bÃ¬nh, Cao
    pe_q1 = np.percentile(y_train, 33.33)
    pe_q2 = np.percentile(y_train, 66.67)
    
    def classify_pe(value):
        if value < pe_q1:
            return 'Thap'
        elif value < pe_q2:
            return 'Trung binh'
        else:
            return 'Cao'
    
    # Chuyá»ƒn Ä‘á»•i y_train vÃ  y_test thÃ nh classification
    y_train_class = np.array([classify_pe(val) for val in y_train])
    y_test_class = np.array([classify_pe(val) for val in y_test])
    
    print(f"   PhÃ¢n loáº¡i PE:")
    print(f"      Tháº¥p: < {pe_q1:.2f} MW ({np.sum(y_train_class == 'Thap')} train, {np.sum(y_test_class == 'Thap')} test)")
    print(f"      Trung bÃ¬nh: {pe_q1:.2f} - {pe_q2:.2f} MW ({np.sum(y_train_class == 'Trung binh')} train, {np.sum(y_test_class == 'Trung binh')} test)")
    print(f"      Cao: >= {pe_q2:.2f} MW ({np.sum(y_train_class == 'Cao')} train, {np.sum(y_test_class == 'Cao')} test)")
    
    # Huáº¥n luyá»‡n Naive Bayes
    nb_model = GaussianNB()
    nb_model.fit(X_train, y_train_class)
    y_pred_class = nb_model.predict(X_test)
    
    # TÃ­nh metrics cho classification
    f1 = f1_score(y_test_class, y_pred_class, average='weighted')
    accuracy = accuracy_score(y_test_class, y_pred_class)
    
    # TÃ­nh metrics cho regression (dá»± Ä‘oÃ¡n giÃ¡ trá»‹ trung bÃ¬nh cá»§a má»—i lá»›p)
    # Äá»ƒ so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh regression khÃ¡c
    class_means = {
        'Thap': np.mean(y_train[y_train_class == 'Thap']) if np.sum(y_train_class == 'Thap') > 0 else pe_q1/2,
        'Trung binh': np.mean(y_train[y_train_class == 'Trung binh']) if np.sum(y_train_class == 'Trung binh') > 0 else (pe_q1 + pe_q2)/2,
        'Cao': np.mean(y_train[y_train_class == 'Cao']) if np.sum(y_train_class == 'Cao') > 0 else (pe_q2 + np.max(y_train))/2
    }
    
    y_pred_regression = np.array([class_means[pred] for pred in y_pred_class])
    nb_metrics_regression = calculate_metrics(y_test, y_pred_regression)
    
    # Káº¿t há»£p metrics
    nb_metrics = {
        'f1_score': f1,
        'accuracy': accuracy,
        'r2': nb_metrics_regression['r2'],
        'rmse': nb_metrics_regression['rmse'],
        'mae': nb_metrics_regression['mae'],
        'mape': nb_metrics_regression['mape'],
        'predictions': y_pred_regression,  # Dá»± Ä‘oÃ¡n dáº¡ng regression Ä‘á»ƒ so sÃ¡nh
        'predictions_class': y_pred_class   # Dá»± Ä‘oÃ¡n dáº¡ng classification
    }
    
    print(f"\nâœ… Naive Bayes (Classification):")
    print(f"    F1 Score: {f1:.4f}")
    print(f"    Accuracy: {accuracy:.4f}")
    print(f"    RÂ² (regression): {nb_metrics_regression['r2']:.4f}")
    print(f"    RMSE (regression): {nb_metrics_regression['rmse']:.4f}")
    
    return nb_metrics, nb_model

def print_comparison_results(dt_metrics, rf_metrics, knn_metrics, cv_results, nb_metrics=None):
    """In káº¿t quáº£ so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh (bá» KNN)"""
    print("\n SO SÃNH HIá»†U SUáº¤T TRÃŠN Táº¬P TEST Tá»T NHáº¤T:")
    print(f"    Decision Tree (tá»‘t nháº¥t):")
    print(f"       RÂ²:   {dt_metrics['r2']:.4f}")
    print(f"       RMSE: {dt_metrics['rmse']:.4f}")
    print(f"       MAE:  {dt_metrics['mae']:.4f}")
    print(f"       MAPE: {dt_metrics['mape']:.2f}%")
    
    print(f"    Random Forest:")
    print(f"       RÂ²:   {rf_metrics['r2']:.4f}")
    print(f"       RMSE: {rf_metrics['rmse']:.4f}")
    print(f"       MAE:  {rf_metrics['mae']:.4f}")
    print(f"       MAPE: {rf_metrics['mape']:.2f}%")
    
    # Bá» KNN
    # print(f"    KNN (tá»‘i Æ°u):")
    # print(f"       RÂ²:   {knn_metrics['r2']:.4f}")
    # print(f"       RMSE: {knn_metrics['rmse']:.4f}")
    # print(f"       MAE:  {knn_metrics['mae']:.4f}")
    # print(f"       MAPE: {knn_metrics['mape']:.2f}%")
    
    if nb_metrics is not None:
        print(f"    Naive Bayes (Classification):")
        print(f"       F1 Score: {nb_metrics['f1_score']:.4f}")
        print(f"       Accuracy: {nb_metrics['accuracy']:.4f}")
        print(f"       RÂ² (regression): {nb_metrics['r2']:.4f}")
        print(f"       RMSE (regression): {nb_metrics['rmse']:.4f}")
        print(f"       MAE (regression): {nb_metrics['mae']:.4f}")
        print(f"       MAPE (regression): {nb_metrics['mape']:.2f}%")
    
    print(f"\nğŸ“Š Káº¾T QUáº¢ CROSS-VALIDATION (5-fold):")
    print(f"    Train RÂ²:     {cv_results['train_r2'].mean():.4f} (Â±{cv_results['train_r2'].std():.4f})")
    print(f"    Test RÂ²:      {cv_results['test_r2'].mean():.4f} (Â±{cv_results['test_r2'].std():.4f})")
    print(f"    Test RMSE:    {cv_results['test_rmse'].mean():.4f} (Â±{cv_results['test_rmse'].std():.4f})")
    
    cv_stability = "Ráº¤T á»”N Äá»ŠNH" if cv_results['test_r2'].std() < 0.02 else "KHÃ á»”N Äá»ŠNH" if cv_results['test_r2'].std() < 0.05 else "CÃ“ BIáº¾N Äá»˜NG"
    print(f"    Äá»™ á»•n Ä‘á»‹nh:    {cv_stability} (Ä‘á»™ lá»‡ch chuáº©n: {cv_results['test_r2'].std():.4f})")